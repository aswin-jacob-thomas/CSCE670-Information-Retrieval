{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2020\n",
    "\n",
    "\n",
    "# Homework 1:  Information Retrieval Basics\n",
    "\n",
    "### 100 points [7% of your final grade]\n",
    "\n",
    "### Due: January 31 (Friday) by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via eCampus (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is collected from Quizlet (https://quizlet.com), a website where users can generated their own flashcards. Each flashcard generated by a user is made up of an entity on the front and a definition describing or explaining the entity correspondingly on the back. We treat entities on each flashcard's front as the queries and the definitions on the back of flashcards as the documents. Definitions (documents) are relevant to an entity (query) if the definitions are from the back of the entity's flashcard; otherwise definitions are not relevant. **In this homework, queries and entities are interchangeable as well as documents and definitions.**\n",
    "\n",
    "The format of the dataset is like this:\n",
    "\n",
    "**query \\t document id \\t document**\n",
    "\n",
    "Examples:\n",
    "\n",
    "decision tree\t\\t 27946 \\t\tshow complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\n",
    "\n",
    "where \"decision tree\" is the entity in the front of a flashcard and \"show complex processes with multiple decision rules.  display decision logic (if statements) as set of (nodes) questions and branches (answers).\" is the definition on the flashcard's back and \"27946\" is the id of the definition. Naturally, this document is relevant to the query.\n",
    "\n",
    "false positive rate\t\\t 686\t\\t fall-out; probability of a false alarm\n",
    "\n",
    "where document 686 is not relevant to query \"decision tree\" because the entity of \"fall-out; probability of a false alarm\" is \"false positive rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parsing (20 points)\n",
    "\n",
    "First, you should tokenize documents (definitions) using **whitespaces and punctuations as delimiters**. Your parser needs to also provide the following three pre-processing options:\n",
    "* Remove stop words: use nltk stop words list (from nltk.corpus import stopwords)\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "* Remove any other strings that you think are less informative or nosiy.\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration options\n",
    "remove_stopwords = True  # or false\n",
    "use_stemming = True # or false\n",
    "remove_otherNoise = True # or false\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9151\n"
     ]
    }
   ],
   "source": [
    "# Your parser function here. It will take the three option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "def get_tokens(input_string):\n",
    "    return re.split(r'\\W+|\\d',input_string)\n",
    "\n",
    "def stem(input_set):\n",
    "    # Takes in a wordset and returns a list of stemmed words\n",
    "    stemmer = PorterStemmer(mode='NLTK_EXTENSIONS')\n",
    "    return [stemmer.stem(word) for word in input_set]\n",
    "\n",
    "def remove_stop_words(input_set):\n",
    "    # Takes in a dictionary of words and returns the list with stopwords removed\n",
    "    return [word for word in input_set if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "\n",
    "def tokenize(remove_stopwords=True,use_stemming=True,remove_otherNoise=True):\n",
    "    vocabulary = []\n",
    "    with open(\"homework_1_data.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            definition = re.findall('.*\\t\\d*\\t(.*)',line)[0].lower()\n",
    "            vocabulary.extend(get_tokens(definition)) #TRY USING NLTK REGEX TOKENIZER\n",
    "        \n",
    "    # Remove stopwords using nltk stopwords data\n",
    "    if remove_stopwords:\n",
    "        vocabulary = remove_stop_words(vocabulary)\n",
    "    \n",
    "    # Use stemming using Porter Stemmer algorithm\n",
    "    if use_stemming:\n",
    "        vocabulary = stem(vocabulary)\n",
    "\n",
    "    # Removing other noise: remove punctuations, remove number, remove single alphabets\n",
    "    if remove_otherNoise:\n",
    "        vocabulary = [word for word in vocabulary if word.isascii() and len(word)>1]\n",
    "        \n",
    "    # Final conversion of the list of words to set of words\n",
    "    vocabulary = set(vocabulary)\n",
    "    vocabulary = vocabulary - {''}\n",
    "    return (len(vocabulary), vocabulary)\n",
    "\n",
    "vocabulary = tokenize(True,True,True)\n",
    "print(vocabulary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* None of pre-processing options      = 15424\n",
    "* remove stop words       = 15284\n",
    "* remove stop words + stemming       = 9170\n",
    "* remove stop words + stemming  + remove other noise     = 9151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Boolean Retrieval (30 points)\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **relational model**, your search engine should treat it as **relational** AND **model**.\n",
    "\n",
    "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
    "* relational database\n",
    "* garbage collection\n",
    "* retrieval model\n",
    "\n",
    "Please use the following format to present your results:\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the index here\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "# Initialize an inverted index\n",
    "# inverted index a map (term -> doc set)\n",
    "# document_map (docID -> (entity, definition))\n",
    "\n",
    "def create_inverted_index():\n",
    "    len_of_dict, dictionary = vocabulary\n",
    "    inverted_index = {}\n",
    "    document_map = {}\n",
    "    \n",
    "    for word in dictionary:\n",
    "        inverted_index[word] = set()\n",
    "\n",
    "    # Each of the dictionary word is initialized with an empty set \n",
    "\n",
    "    # Iterate over each document\n",
    "    # Tokenize the terms from the document\n",
    "    # Add the docID to each of the term set\n",
    "\n",
    "    with open(\"homework_1_data.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            (entity, docID, definition) = re.findall('(.*)\\t(\\d*)\\t(.*)',line)[0]\n",
    "            docID = int(docID)\n",
    "            \n",
    "            # Creating the document map\n",
    "#             document_map[docID] = (entity, definition)\n",
    "#             print(get_tokens(definition.lower))\n",
    "            # Now parse the definition to create tokens from each definition\n",
    "    \n",
    "    \n",
    "            tokens = stem(remove_stop_words(get_tokens(definition.lower())))\n",
    "            document_map[docID] = (entity, definition, tokens)\n",
    "            tokens = set(tokens)\n",
    "            \n",
    "            for word in tokens:\n",
    "                if(word in inverted_index):\n",
    "                    inverted_index[word].add(docID)\n",
    "\n",
    "    return inverted_index, document_map\n",
    "\n",
    "inverted_index, document_map = create_inverted_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  relational database\n",
      "result  1 :\n",
      "entity:  database management system\n",
      "definition id:  654\n",
      "definition:  dbms allows users to create, read, update, and delete structured data in a relational database. managers send requests to dbms and the dbms performs manipulation of the data. can retrieve information from using sql or qbe (query by example).   relational database management system: allows users to create, read, update, and delete data in a relational database.  pros: increased flexibility, inc scalability and performance, reduced info redundancy, inc info integrity/quality, increased info security.\n",
      "result  2 :\n",
      "entity:  database management system\n",
      "definition id:  657\n",
      "definition:  general hospital utilizes various related files that include clinical and financial data to generate reports such as ms drg case mix reports. what application would be most effective for this activity  desktop publishing  word processing database management system command interpreter\n",
      "result  3 :\n",
      "entity:  database management system\n",
      "definition id:  682\n",
      "definition:  database management system software that controls how related collections od data are stored.\n",
      "result  4 :\n",
      "entity:  relational model\n",
      "definition id:  741\n",
      "definition:  developed by ef codd of ibm in 19070, the relational model is based on mathematical set theory and represents data as independent relations. each relation (table) is conceptually represented as a two dimensional structure of intersecting rows and columns. the relations are related to each other through the sharing of common entity characteristics (values in columns). -to use an analogy, it produced an &\"automatic transmission&\" database to replace the &\"standard transmission&\" databases that preceded it. -describes a precise set of data manipulation constructs based on advanced mathematical concepts.\n",
      "result  5 :\n",
      "entity:  relational model\n",
      "definition id:  750\n",
      "definition:  all relational database dbms products are built on this model -e.f. codd applied the concepts of a branch of math called relational algebra to the problem of databases\n",
      "\n",
      "query:  garbage collection\n",
      "result  1 :\n",
      "entity:  garbage collector\n",
      "definition id:  4150\n",
      "definition:  the part of the operating system that performs garbage collection.\n",
      "result  2 :\n",
      "entity:  memory safety\n",
      "definition id:  13115\n",
      "definition:  memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result  3 :\n",
      "entity:  garbage collection\n",
      "definition id:  21550\n",
      "definition:  automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "result  4 :\n",
      "entity:  garbage collection\n",
      "definition id:  21553\n",
      "definition:  garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result  5 :\n",
      "entity:  garbage collection\n",
      "definition id:  21554\n",
      "definition:  there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "\n",
      "query:  retrieval model\n",
      "result  1 :\n",
      "entity:  data model\n",
      "definition id:  6983\n",
      "definition:  - a collection of concepts that can be used to describe the structure of a database - describes the structure - dbms is based on a data model   structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)  types of data models: - high-level (conceptual i.e. er) - low level (physical i.e. xml) - implementation (representational) combines conceptual and physical i.e. relational model - nosql data models i.e. column, key-value, document stores\n",
      "result  2 :\n",
      "entity:  data model\n",
      "definition id:  7031\n",
      "definition:  \\ structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)\n",
      "result  3 :\n",
      "entity:  query language\n",
      "definition id:  9085\n",
      "definition:  used to update and retrieve data that is stored in a data model\n",
      "result  4 :\n",
      "entity:  physical design\n",
      "definition id:  10327\n",
      "definition:  translate logical model into technical specifications for storing/retrieving data, store data to achieve efficiency and quality\n",
      "result  5 :\n",
      "entity:  physical design\n",
      "definition id:  10335\n",
      "definition:  translate logical model into technical specifications for storing / retrieving data. store data to achieve efficiency and quality\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['retriev', 'model'],\n",
       " [6983,\n",
       "  7031,\n",
       "  9085,\n",
       "  10327,\n",
       "  10335,\n",
       "  10341,\n",
       "  17315,\n",
       "  17705,\n",
       "  19703,\n",
       "  19708,\n",
       "  19727,\n",
       "  19731,\n",
       "  19733])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for the input using your index and print out ids of matching documents.\n",
    "\n",
    "def get_postings_list(query_word):\n",
    "    if(query_word in inverted_index):\n",
    "        return inverted_index[query_word]\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "def print_output(query, result_set):\n",
    "    print('query: ',query)\n",
    "    for i,result in enumerate(result_set[:5]):\n",
    "        entity, definition, list_of_words = document_map[result]\n",
    "        print('result ',i+1,':')\n",
    "        print('entity: ', entity)\n",
    "        print('definition id: ', result)\n",
    "        print('definition: ', definition)\n",
    "\n",
    "# Takes in a query and returns the intersection of postings list \n",
    "def query_inverted_index(query, printOutput=False):\n",
    "    query_list = stem(remove_stop_words(get_tokens(query)))\n",
    "    setlist = [get_postings_list(query) for query in query_list]\n",
    "    result_set = sorted(list(set.intersection(*setlist)))\n",
    "    \n",
    "    if(printOutput):\n",
    "        print_output(query, result_set)\n",
    "    \n",
    "    return (query_list, result_set)\n",
    "\n",
    "query_inverted_index(\"relational database\",True)   \n",
    "print()\n",
    "query_inverted_index(\"garbage collection\",True)\n",
    "print()\n",
    "query_inverted_index(\"retrieval model\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Could your boolean search engine find relevant documents for these queries? What is the impact of the three pre-processing options? Do they improve your search quality?\n",
    "\n",
    "Answer: Boolean retrieval finds most of the relevant documents for the queries. But since it is not taking the ranking of the documents into consideration, users are not getting the best results. For instance, for the query, \"relational database\", boolean retrieval is not returning the best document returned by TF-IDF model out of the 5 documents returned. \n",
    "Yes,preprocessing improves the search quality considerably. For instance, without stemming, no documents are returned for the query \"retrieval model\". But with stemming, more accurate results are obtained. The reason is that many words like retrievals, retrieve, retrieving gets converted to retriev. This helps the model to return document matches. Also preprocessing the words reduces the size of the vocabulary by over 40%. This will greatly improve the latency of the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Ranking Documents (50 points) \n",
    "\n",
    "In this part, your job is to rank the documents that have been retrieved by the Boolean Retrieval component in Part 2, according to their relevance with each query.\n",
    "\n",
    "### A: Ranking with simple sums of TF-IDF scores (15 points) \n",
    "For a multi-word query, we rank documents by a simple sum of the TF-IDF scores for the query terms in the document.\n",
    "TF is the log-weighted term frequency $1+log(tf)$; and IDF is the log-weighted inverse document frequency $log(\\frac{N}{df})$\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 results plus the TF-IDF sum score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  relational database\n",
      "result  1 :\n",
      "score:  4.71733880527531\n",
      "entity:  relational algebra\n",
      "definition id:  7156\n",
      "definition:  - a theoretical language with operations that work on one or more relations to define another relation without changing the original relation(s)  - relation-at-a-time (or set) language in which all tuples, possibly from several relations, are manipulated in one statement without looping  relational algebra, first created by edgar f. codd while at ibm, is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it.  the main application of relational algebra is providing a theoretical foundation for relational databases, particularly query languages for such databases, chief among which is sql.\n",
      "result  2 :\n",
      "score:  4.357658330802902\n",
      "entity:  relational database\n",
      "definition id:  28378\n",
      "definition:  a type of database system where data is stored in  tables related by common fields. a relational database is the most common type of database used with a personal computer. similar data is held in a table (e.g. students, courses, books, instructors) and tables are related through a common field (a field that is in more than one table) microsoft access and corel paradox are both relational database management systems.\n",
      "result  3 :\n",
      "score:  4.238364518320238\n",
      "entity:  relational database\n",
      "definition id:  28254\n",
      "definition:  finite set of relations​. each relation consists of a schema and an instance​. database schema = set of relation schemas constraints among relations (inter-relational constraints)​. database instance = set of (corresponding) relation instances\n",
      "result  4 :\n",
      "score:  4.122275123103649\n",
      "entity:  relational model\n",
      "definition id:  741\n",
      "definition:  developed by ef codd of ibm in 19070, the relational model is based on mathematical set theory and represents data as independent relations. each relation (table) is conceptually represented as a two dimensional structure of intersecting rows and columns. the relations are related to each other through the sharing of common entity characteristics (values in columns). -to use an analogy, it produced an &\"automatic transmission&\" database to replace the &\"standard transmission&\" databases that preceded it. -describes a precise set of data manipulation constructs based on advanced mathematical concepts.\n",
      "result  5 :\n",
      "score:  4.017820665941266\n",
      "entity:  database management system\n",
      "definition id:  654\n",
      "definition:  dbms allows users to create, read, update, and delete structured data in a relational database. managers send requests to dbms and the dbms performs manipulation of the data. can retrieve information from using sql or qbe (query by example).   relational database management system: allows users to create, read, update, and delete data in a relational database.  pros: increased flexibility, inc scalability and performance, reduced info redundancy, inc info integrity/quality, increased info security.\n",
      "\n",
      "query:  garbage collection\n",
      "result  1 :\n",
      "score:  6.530031517175487\n",
      "entity:  garbage collection\n",
      "definition id:  21553\n",
      "definition:  garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result  2 :\n",
      "score:  6.3292301409376925\n",
      "entity:  garbage collection\n",
      "definition id:  21550\n",
      "definition:  automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "result  3 :\n",
      "score:  6.3292301409376925\n",
      "entity:  garbage collection\n",
      "definition id:  21554\n",
      "definition:  there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "result  4 :\n",
      "score:  5.520628775347339\n",
      "entity:  garbage collection\n",
      "definition id:  21559\n",
      "definition:  garbage collection is a feature that automatically deletes unused memory that is no longer being referenced. you cannot force it but you can request it using system.gc() but you should never use that because it slows down your program heavily. once the system thinks an object it ready for collection, it will call finalize() on it which does final cleanup and prepares it do be collected\n",
      "result  5 :\n",
      "score:  4.864784180250639\n",
      "entity:  garbage collector\n",
      "definition id:  4150\n",
      "definition:  the part of the operating system that performs garbage collection.\n",
      "\n",
      "query:  retrieval model\n",
      "result  1 :\n",
      "score:  4.29230470611025\n",
      "entity:  data model\n",
      "definition id:  6983\n",
      "definition:  - a collection of concepts that can be used to describe the structure of a database - describes the structure - dbms is based on a data model   structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)  types of data models: - high-level (conceptual i.e. er) - low level (physical i.e. xml) - implementation (representational) combines conceptual and physical i.e. relational model - nosql data models i.e. column, key-value, document stores\n",
      "result  2 :\n",
      "score:  3.3370417172064872\n",
      "entity:  data model\n",
      "definition id:  7031\n",
      "definition:  \\ structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)\n",
      "result  3 :\n",
      "score:  3.3370417172064872\n",
      "entity:  query language\n",
      "definition id:  9085\n",
      "definition:  used to update and retrieve data that is stored in a data model\n",
      "result  4 :\n",
      "score:  3.3370417172064872\n",
      "entity:  physical design\n",
      "definition id:  10327\n",
      "definition:  translate logical model into technical specifications for storing/retrieving data, store data to achieve efficiency and quality\n",
      "result  5 :\n",
      "score:  3.3370417172064872\n",
      "entity:  physical design\n",
      "definition id:  10335\n",
      "definition:  translate logical model into technical specifications for storing / retrieving data. store data to achieve efficiency and quality\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# hint: you could first call boolean retrieval function in part 2 to find possible relevant documents, \n",
    "# and then rank these documents in this part. Hence, you don't need to rank all documents.\n",
    "total_collection_length = len(document_map)\n",
    "def tf_idf(query):\n",
    "    # First call query_inverted_index on the query to return the list of document ids\n",
    "    # Each document id can then be parsed to obtain the tf\n",
    "    # idf value of the term depends on the entire collection of documents\n",
    "    query_list, docIDs = query_inverted_index(query)\n",
    "    query_list = set(query_list)\n",
    "    \n",
    "    query_occuring_docs = [len(get_postings_list(query)) for query in query_list]\n",
    "    \n",
    "    # IDF value of each of the query term in the given query\n",
    "    idf_list = np.log10(total_collection_length/np.array(query_occuring_docs))\n",
    "    # Get the number of times the term occurs in each document\n",
    "    \n",
    "    # Create docID - weight map\n",
    "    doc_weight_map = {}\n",
    "    doc_vector_map = {}\n",
    "    \n",
    "    for docID in docIDs:\n",
    "        entity, definition, list_of_words = document_map[docID]\n",
    "        tf_list = [list_of_words.count(query) for query in query_list]\n",
    "        tf_list = np.array(tf_list)\n",
    "        tf_list = 1+np.log10(tf_list)\n",
    "        tf_idf_weight = tf_list * idf_list\n",
    "        doc_vector_map[docID] = tf_idf_weight\n",
    "        weight_value = np.sum(tf_idf_weight)\n",
    "        \n",
    "        doc_weight_map[docID] = weight_value\n",
    "        \n",
    "    doc_weight_map = dict(sorted(doc_weight_map.items(), key=lambda x: (-x[1], x[0])))\n",
    "    \n",
    "    return doc_weight_map, doc_vector_map\n",
    "\n",
    "def print_output(query, result_set):\n",
    "    print('query: ',query)\n",
    "    for i,result in enumerate(result_set[:5]):\n",
    "        docID,score = result\n",
    "        entity, definition, list_of_words = document_map[docID]\n",
    "        print('result ',i+1,':')\n",
    "        print('score: ', score)\n",
    "        print('entity: ', entity)\n",
    "        print('definition id: ', docID)\n",
    "        print('definition: ', definition)\n",
    "\n",
    "def print_top_tf_idf_docs(query, isPrint=True):\n",
    "    top_results,_ = tf_idf(query)\n",
    "    top_results = list(top_results.items())[:5]\n",
    "    if isPrint:\n",
    "        print_output(query, top_results)\n",
    "    return top_results\n",
    "    \n",
    "tfidf_1 = print_top_tf_idf_docs(\"relational database\")\n",
    "print()\n",
    "tfidf_2 = print_top_tf_idf_docs(\"garbage collection\")\n",
    "print()\n",
    "tfidf_3 = print_top_tf_idf_docs(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Ranking with vector space model with TF-IDF (15 points) \n",
    "\n",
    "**Cosine:** You should use cosine as your scoring function. \n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TF-IDF scores as introduced in A. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 documents plus the cosine score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  relational database\n",
      "result  1 :\n",
      "score:  0.9998918859021982\n",
      "entity:  database systems\n",
      "definition id:  15793\n",
      "definition:  to briefly reiterate this content, in a database system, data from related applications is pooled together in logically related files (the database) that can be accessed by multiple applications through a database management system (dbms).\n",
      "result  2 :\n",
      "score:  0.9998918859021982\n",
      "entity:  relational database\n",
      "definition id:  28199\n",
      "definition:  a database that represents data as a collection of tables in which all data relationships are represented by common values in related tables. if you want your application to handle a lot of complicated querying, database transactions and routine analysis of data, you'll probably want to stick with a relational database\n",
      "result  3 :\n",
      "score:  0.9998918859021982\n",
      "entity:  relational database\n",
      "definition id:  28307\n",
      "definition:  a collection of related tabes. establishes the relationships between entities by means of a common field. the tables are constructed so that there is a logical link between them.  -the difference between a database and a relational database is in the way the tables are constructed. -a series of excel spreadsheets, just a database.\n",
      "result  4 :\n",
      "score:  0.9985798636532639\n",
      "entity:  relational model\n",
      "definition id:  770\n",
      "definition:  - produced an automatic transmission database that replaced standard transmission databases - based on a relation (intersecting tuples [rows] and attributes [columns]) - describes a precise set of data manipulation  constructs\n",
      "result  5 :\n",
      "score:  0.9985798636532639\n",
      "entity:  relational model\n",
      "definition id:  788\n",
      "definition:  mimics the structure of a relational database closely enough that you can actually sit down and start building the database.\n",
      "\n",
      "query:  garbage collection\n",
      "result  1 :\n",
      "score:  0.9667735087329943\n",
      "entity:  garbage collection\n",
      "definition id:  21559\n",
      "definition:  garbage collection is a feature that automatically deletes unused memory that is no longer being referenced. you cannot force it but you can request it using system.gc() but you should never use that because it slows down your program heavily. once the system thinks an object it ready for collection, it will call finalize() on it which does final cleanup and prepares it do be collected\n",
      "result  2 :\n",
      "score:  0.917036487814855\n",
      "entity:  garbage collector\n",
      "definition id:  4150\n",
      "definition:  the part of the operating system that performs garbage collection.\n",
      "result  3 :\n",
      "score:  0.917036487814855\n",
      "entity:  memory safety\n",
      "definition id:  13115\n",
      "definition:  memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result  4 :\n",
      "score:  0.917036487814855\n",
      "entity:  garbage collection\n",
      "definition id:  21550\n",
      "definition:  automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "result  5 :\n",
      "score:  0.917036487814855\n",
      "entity:  garbage collection\n",
      "definition id:  21554\n",
      "definition:  there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "\n",
      "query:  retrieval model\n",
      "result  1 :\n",
      "score:  0.9966624774349581\n",
      "entity:  data model\n",
      "definition id:  6983\n",
      "definition:  - a collection of concepts that can be used to describe the structure of a database - describes the structure - dbms is based on a data model   structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)  types of data models: - high-level (conceptual i.e. er) - low level (physical i.e. xml) - implementation (representational) combines conceptual and physical i.e. relational model - nosql data models i.e. column, key-value, document stores\n",
      "result  2 :\n",
      "score:  0.9840271900981782\n",
      "entity:  data model\n",
      "definition id:  7031\n",
      "definition:  \\ structure of data model:  - records - types - relationships - constraints - basic operations (specifying retrievals and updates)\n",
      "result  3 :\n",
      "score:  0.9840271900981782\n",
      "entity:  query language\n",
      "definition id:  9085\n",
      "definition:  used to update and retrieve data that is stored in a data model\n",
      "result  4 :\n",
      "score:  0.9840271900981782\n",
      "entity:  physical design\n",
      "definition id:  10327\n",
      "definition:  translate logical model into technical specifications for storing/retrieving data, store data to achieve efficiency and quality\n",
      "result  5 :\n",
      "score:  0.9840271900981782\n",
      "entity:  physical design\n",
      "definition id:  10335\n",
      "definition:  translate logical model into technical specifications for storing / retrieving data. store data to achieve efficiency and quality\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def vector_space(query):\n",
    "    doc_weight, doc_vector = tf_idf(query)\n",
    "    # We only need to vector and find the cosine value from the vector\n",
    "    cosine_dict = {}\n",
    "    for docID in doc_vector:\n",
    "        query_list = stem(remove_stop_words(get_tokens(query)))\n",
    "        \n",
    "        query_vector = [query_list.count(query) for query in vocabu]\n",
    "        document_vector = doc_vector[docID]\n",
    "        \n",
    "        cosine_val = np.dot(query_vector, document_vector)/(np.linalg.norm(query_vector) * np.linalg.norm(document_vector))\n",
    "        cosine_dict[docID] = cosine_val\n",
    "    \n",
    "    cosine_dict = dict(sorted(cosine_dict.items(), key=lambda x: (-x[1], x[0])))\n",
    "    return cosine_dict\n",
    "\n",
    "def print_top_vsm_docs(query, isPrint=True):\n",
    "    top_results = vector_space(query)\n",
    "    top_results = list(top_results.items())[:5]\n",
    "    if isPrint:\n",
    "        print_output(query, top_results)\n",
    "    return top_results\n",
    "\n",
    "vsm_1 =print_top_vsm_docs(\"relational database\")\n",
    "print()\n",
    "vsm_2 = print_top_vsm_docs(\"garbage collection\")\n",
    "print()\n",
    "vsm_3 = print_top_vsm_docs(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C: Ranking with BM25 (20 points) \n",
    "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
    "\n",
    "**Output:**\n",
    "For each given query in Part 2, you should just rank the documents retrieved by your boolean search. You only need to output the top-5 documents plus the BM25 score of each of these documents. Please use the following format to present your results:\n",
    "\n",
    "* query: relational database\n",
    "* result 1:\n",
    "* score: 0.1\n",
    "* entity: database management system\n",
    "* definition id: 656\n",
    "* definition: software system used to manage databases\n",
    "* result 2:\n",
    "* ......\n",
    "* query: garbage collection\n",
    "* ......\n",
    "* query: retrieval model\n",
    "* ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  relational database\n",
      "result  1 :\n",
      "score:  4.502180318551657\n",
      "entity:  relational database\n",
      "definition id:  28234\n",
      "definition:  a database that is modeled using the relational database model a collection of related relations within which each relation has a unique name\n",
      "result  2 :\n",
      "score:  4.445402555237603\n",
      "entity:  relational database\n",
      "definition id:  28254\n",
      "definition:  finite set of relations​. each relation consists of a schema and an instance​. database schema = set of relation schemas constraints among relations (inter-relational constraints)​. database instance = set of (corresponding) relation instances\n",
      "result  3 :\n",
      "score:  4.025605455645353\n",
      "entity:  relational database\n",
      "definition id:  28260\n",
      "definition:  form of database that has more than one relational table - each table is related to the other table\n",
      "result  4 :\n",
      "score:  4.005014972341211\n",
      "entity:  relational database\n",
      "definition id:  28312\n",
      "definition:  a collection of related relations in which each relation has a unique name  operational/transactional databases\n",
      "result  5 :\n",
      "score:  3.954656567996718\n",
      "entity:  relational database\n",
      "definition id:  28205\n",
      "definition:  a database built using the relational database model\n",
      "\n",
      "query:  garbage collection\n",
      "result  1 :\n",
      "score:  6.459251834847479\n",
      "entity:  garbage collection\n",
      "definition id:  21553\n",
      "definition:  garbage collection (gc) is a form of automatic memory management. the garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program.\n",
      "result  2 :\n",
      "score:  5.97735814075639\n",
      "entity:  garbage collector\n",
      "definition id:  4150\n",
      "definition:  the part of the operating system that performs garbage collection.\n",
      "result  3 :\n",
      "score:  5.894456088150926\n",
      "entity:  garbage collection\n",
      "definition id:  21554\n",
      "definition:  there is a third phase of 2pc called garbage collection. replicas must retain records of past transactions, just in case leader fails. in practice, leader periodically tells replicas to garbage collect.\n",
      "result  4 :\n",
      "score:  5.16829732790815\n",
      "entity:  memory safety\n",
      "definition id:  13115\n",
      "definition:  memory management handled differently such that there is garbage collection, prevent dangling pointer references.\n",
      "result  5 :\n",
      "score:  4.862364290922045\n",
      "entity:  garbage collection\n",
      "definition id:  21550\n",
      "definition:  automatic memory management is made possible by garbage collection in .net framework. when a class object is created at runtime, certain memory space is allocated to it in the heap memory. however, after all the actions related to the object are completed in the program, the memory space allocated to it is a waste as it cannot be used. in this case, garbage collection is very useful as it automatically releases the memory space after it is no longer required.\n",
      "\n",
      "query:  retrieval model\n",
      "result  1 :\n",
      "score:  3.9808845705893283\n",
      "entity:  query language\n",
      "definition id:  9085\n",
      "definition:  used to update and retrieve data that is stored in a data model\n",
      "result  2 :\n",
      "score:  3.9808845705893283\n",
      "entity:  online analytical processing\n",
      "definition id:  19727\n",
      "definition:  tools for retrieving, processing, and modelling data from the data warehouse\n",
      "result  3 :\n",
      "score:  3.9808845705893283\n",
      "entity:  online analytical processing\n",
      "definition id:  19731\n",
      "definition:  enable retrieving, processing, and modeling data from the data warehouse\n",
      "result  4 :\n",
      "score:  3.7942806063429533\n",
      "entity:  online analytical processing\n",
      "definition id:  19708\n",
      "definition:  olap - tools for retrieving, processing, and modeling data from the data warehouse\n",
      "result  5 :\n",
      "score:  3.326492586382864\n",
      "entity:  online analytical processing\n",
      "definition id:  19733\n",
      "definition:  a set of tools that provide advanced data analysis for retrieving, processing, and modeling data from the data warehouse\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def bm25(query):\n",
    "    D = total_collection_length\n",
    "    avgdl = 0\n",
    "    query_list, docIDs = query_inverted_index(query)\n",
    "    query_list = set(query_list)\n",
    "    doc_weight_map = {}\n",
    "    \n",
    "    for docID in docIDs:\n",
    "        avgdl += len(set(document_map[docID][2]))\n",
    "    avgdl /= len(docIDs)\n",
    "    query_occuring_docs = [len(get_postings_list(query)) for query in query_list]\n",
    "    query_occuring_docs = np.array(query_occuring_docs)\n",
    "#     idf_list = np.log10((total_collection_length-query_occuring_docs+0.5)/(query_occuring_docs+0.5))\n",
    "    idf_list = np.log10(total_collection_length/np.array(query_occuring_docs))\n",
    "    for docID in docIDs:\n",
    "        entity, definition, list_of_words = document_map[docID]\n",
    "        tf_list = [list_of_words.count(query) for query in query_list]\n",
    "        D = len(set(list_of_words))\n",
    "        tf_list = np.array(tf_list)\n",
    "        tf_list = tf_list*2.2/(tf_list+1.2*(0.25+D/avgdl))\n",
    "        tf_idf_weight = tf_list * idf_list\n",
    "        weight_value = np.sum(tf_idf_weight)\n",
    "        \n",
    "        doc_weight_map[docID] = weight_value\n",
    "        \n",
    "    doc_weight_map = dict(sorted(doc_weight_map.items(), key=lambda x: (-x[1], x[0])))\n",
    "    \n",
    "    return doc_weight_map\n",
    "\n",
    "def print_top_bm25_docs(query, isPrint=True):\n",
    "    top_results = bm25(query)\n",
    "    top_results = list(top_results.items())[:5]\n",
    "    if isPrint:\n",
    "        print_output(query, top_results)\n",
    "    \n",
    "    return top_results\n",
    "    \n",
    "bm_1 = print_top_bm25_docs(\"relational database\")\n",
    "print()\n",
    "bm_2 = print_top_bm25_docs(\"garbage collection\")\n",
    "print()\n",
    "bm_3 = print_top_bm25_docs(\"retrieval model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Briefly discuss the differences you see between the three methods. Is there one you prefer?\n",
    "\n",
    "Answer: The main issue with all the three models is that all of them considers documents as bag of words and not considering the relative positions of the query terms. This factor can greatly alter the ranking of the documents. \n",
    "\n",
    "Vector Space Model is the most intuitive out of all three models. Since each document and query is considered as a vector, finding the similarity between the vectors provides an inherent idea of the relevance. It is also scaled between 0 and 1. This normalization also helps in visualizing the relevance of the document. BM25 gave the best precision@5 out of the three methods, but it is hard to visualize the scoring idea behind it.\n",
    "\n",
    "TF-IDF model does not take into consideration the frequence of the terms in the query while Vector Space Model does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Evaluation (10 points)\n",
    "Rather than just compare methods by pure observation, there are several metrics to evaluate the performance of an IR engine: Precision, Recall, MAP, NDCG, HitRate and so on. These all require a ground truth set of queries and documents with a notion of **relevance**. These ground truth judgments can be expensive to obtain, so we are cutting corners here and treating a flashcard's front and back as a \"relevant\" query-document pair.\n",
    "\n",
    "That is, if a document (definition) in your top-5 results is from the back of query's (entity's) flashcard, this document is regarded as relevant to the query (entity). This document is also called a hit in IR. Based on the ground-truth, you could calculate the metrics for the three ranking methods and provide the results like these:\n",
    "\n",
    "* metric: Precision@5\n",
    "* TF-IDF - score1\n",
    "* Vector Space Model with TF-IDF - score2\n",
    "* BM25 - score3\n",
    "\n",
    "You could pick any of the reasonable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Score  0.4000000000000001\n",
      "Vector Space Model with TF-IDF Score  0.3333333333333333\n",
      "BM25 Score  0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def evaluation():\n",
    "    # TF-IDF Form\n",
    "    tf_rb = np.sum(np.array([document_map[docID][0] == \"relational database\" for docID,_ in tfidf_1]))/5\n",
    "    tf_gc = np.sum(np.array([document_map[docID][0] == \"garbage collection\" for docID,_ in tfidf_2]))/5\n",
    "    tf_rm = np.sum(np.array([document_map[docID][0] == \"retrieval model\" for docID,_ in tfidf_2]))/5\n",
    "    print(\"TF-IDF Score \", (tf_rb+tf_rm+tf_gc)/3)\n",
    "    \n",
    "    # Vector Space Model with TF-IDF\n",
    "    vsm_rb = np.sum(np.array([document_map[docID][0] == \"relational database\" for docID,_ in vsm_1]))/5\n",
    "    vsm_gc = np.sum(np.array([document_map[docID][0] == \"garbage collection\" for docID,_ in vsm_2]))/5\n",
    "    vsm_rm = np.sum(np.array([document_map[docID][0] == \"retrieval model\" for docID,_ in vsm_3]))/5\n",
    "    print(\"Vector Space Model with TF-IDF Score \", (vsm_rb+vsm_rm+vsm_gc)/3)\n",
    "    \n",
    "    # BM25\n",
    "    bm_rb = np.sum(np.array([document_map[docID][0] == \"relational database\" for docID,_ in bm_1]))/5\n",
    "    bm_gc = np.sum(np.array([document_map[docID][0] == \"garbage collection\" for docID,_ in bm_2]))/5\n",
    "    bm_rm = np.sum(np.array([document_map[docID][0] == \"retrieval model\" for docID,_ in bm_3]))/5\n",
    "    print(\"BM25 Score \", (bm_rb+bm_rm+bm_gc)/3)\n",
    "evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** You should fill out your collaboration declarations here.**\n",
    "\n",
    "**Reminder:** You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by filling out the Collaboration Declarations at the bottom of this notebook.\n",
    "\n",
    "Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
